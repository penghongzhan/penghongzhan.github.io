---
title: 倒排索引保存的信息
date: 2019-10-16 21:05:31
tags:
- elasticsearch
- es
- elastic
- 倒排索引
categories:
- elasticsearch
---

# 需要记录的信息

- 词在文档中出现的频率：文档列表中每个文档id对应一个值
- 出现该次的文档的数量：文档列表的长度
- 高亮搜索结果：词在文档中的偏移信息(offset)，包括起始位置和长度
- 短语查询：记录没个词的position信息，如果查询brown fox，还是会按照分词的term（brown和fox）进行查询，查询到的结果发现，brown和fox都对应同一个文档，而且在文档中的position是连续的，所以该文档中包含该短语（brown fox）
- 如果用户希望在Term级别干预查询打分结果，那么就需要对文档中的每个词存储额外的信息（payload）

综上，倒排索引需要存储的信息主要有以下几方面：

- 词（Term）
- 倒排文档列表（DocIDList）
- 词频（TermFreq）
- Position
- Offset
- Payload

同一个Term在同一个Document的同一个Field中，Position、Offset、Payload可能会出现多次，次数由TermFreq决定。倒排索引的逻辑结构如下所示：

```
|+ field1(name,type)
    |+ term1
        |+ doc1
            |+ termFreq = 2
                |+ [position1,offset1,payload1]
                |+ [position2,offset2,payload2]
        |+ doc2
            |+ termFreq = 1
                |+ [position3,offset3,payload3]
        |+...
    |+ term2
    |+...
|+ field2(name,type)
|+ ...
```

# 对应的lucene文件

> 参考印象笔记文章：Elasticsearch存储深入详解 - 铭毅天下（公众号同名） - CSDN博客

当文档很少的时候，好像只会有一个segment后缀的文件

## 倒排索引文件

```
 12K Oct 10 17:25 _b_Lucene50_0.tip
2.1M Oct 10 17:25 _b_Lucene50_0.tim
172K Oct 10 17:25 _b_Lucene50_0.doc
143K Oct 10 17:25 _b_Lucene50_0.pos
  78 Oct 10 17:25 _b.nvm
  59 Oct 10 17:25 _b.nvd
```

## 原始文档文件

```
1.7K Oct 10 17:25 _b.fnm
2.0K Oct 10 17:24 _b.fdx
6.5M Oct 10 17:24 _b.fdt
```

## 正排索引文件

```
 803 Oct 10 17:25 _b_Lucene54_0.dvm
786K Oct 10 17:25 _b_Lucene54_0.dvd
```

> 存储原文_source的文件.fdt .fdm .fdx; </br>
> 存储倒排索引的文件.tim .tip .doc;</br>
> 用于聚合排序的列存文件.dvd .dvm; </br>
> 全文检索文件.pos .pay .nvd .nvm等;</br>
> 加载到内存中的文件有.fdx .tip .dvm</br>

另外segment较小时文件内容是保存在.cfs文件中，.cfe文件保存Lucene各文件在.cfs文件的位置信息，这是为了减少Lucene打开的文件句柄数。

倒排索引保存的信息可以通过index_options进行设置

- docs：出现的文档
- freqs：在文档中的频次
- positions：文档中出现的位置
- offsets：文档中偏移量

中间两个是用于评分的时候用的，第四个用于高亮。text字段默认开启到position，keyword字段只保存docs

# 短语查询

一个被认定为和短语 quick brown fox 匹配的文档，必须满足以下这些要求：

- quick、brown 和 fox 需要全部出现在域中。
- brown 的位置应该比 quick 的位置大 1 。
- fox 的位置应该比 quick 的位置大 2 。

如果以上任何一个选项不成立，则该文档不能认定为匹配。上面提到的位置信息就是用到`.pos`文件中的信息

# 查看分词结果

```
GET /_analyze?analyzer=standard
Quick brown fox

{
    "tokens": [
        {
            "token": "quick",
            "start_offset": 0,
            "end_offset": 5,
            "type": "<ALPHANUM>",
            "position": 0
        },
        {
            "token": "brown",
            "start_offset": 6,
            "end_offset": 11,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "fox",
            "start_offset": 12,
            "end_offset": 15,
            "type": "<ALPHANUM>",
            "position": 2
        }
    ]
}
```

